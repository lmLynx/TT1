{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from PIL import Image # Usaremos PIL solo para cargar y procesar la imagen. Es el mínimo indispensable.\n",
    "\n",
    "# --- Funciones de Activación ---\n",
    "# Las funciones de activación introducen no-linealidad, permitiendo a la red aprender patrones complejos.\n",
    "\n",
    "def relu(x):\n",
    "    \"\"\"Función de activación ReLU.\"\"\"\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def relu_derivative(x):\n",
    "    \"\"\"Derivada de ReLU para la retropropagación.\"\"\"\n",
    "    return np.where(x > 0, 1, 0)\n",
    "\n",
    "def sigmoid(x):\n",
    "    \"\"\"Función de activación Sigmoid (útil para la capa de salida del autoencoder).\"\"\"\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    \"\"\"Derivada de Sigmoid.\"\"\"\n",
    "    s = sigmoid(x)\n",
    "    return s * (1 - s)\n",
    "\n",
    "# --- Clase para una Capa Densa ---\n",
    "# Esta es la unidad de construcción fundamental de nuestra red.\n",
    "class DenseLayer:\n",
    "    def __init__(self, input_size, output_size, activation='relu'):\n",
    "        # Inicializamos los pesos con valores pequeños y aleatorios y los sesgos en cero.\n",
    "        self.weights = np.random.randn(input_size, output_size) * 0.01\n",
    "        self.biases = np.zeros((1, output_size))\n",
    "        self.activation_name = activation\n",
    "        \n",
    "        # Variables para guardar valores durante el forward pass para usarlos en el backward pass\n",
    "        self.input = None\n",
    "        self.z = None # Salida lineal (antes de la activación)\n",
    "\n",
    "    def forward(self, input_data):\n",
    "        \"\"\"Paso hacia adelante: calcula la salida de la capa.\"\"\"\n",
    "        self.input = input_data\n",
    "        self.z = np.dot(self.input, self.weights) + self.biases\n",
    "        \n",
    "        if self.activation_name == 'relu':\n",
    "            return relu(self.z)\n",
    "        elif self.activation_name == 'sigmoid':\n",
    "            return sigmoid(self.z)\n",
    "        return self.z # Sin activación para la capa de salida del clasificador\n",
    "\n",
    "    def backward(self, output_error, learning_rate):\n",
    "        \"\"\"Paso hacia atrás: calcula los gradientes y actualiza los pesos.\"\"\"\n",
    "        if self.activation_name == 'relu':\n",
    "            delta = output_error * relu_derivative(self.z)\n",
    "        elif self.activation_name == 'sigmoid':\n",
    "            delta = output_error * sigmoid_derivative(self.z)\n",
    "        else:\n",
    "            delta = output_error\n",
    "\n",
    "        # Gradiente de los pesos y sesgos\n",
    "        weights_gradient = np.dot(self.input.T, delta)\n",
    "        biases_gradient = np.sum(delta, axis=0, keepdims=True)\n",
    "        \n",
    "        # Gradiente de entrada para la capa anterior\n",
    "        input_error = np.dot(delta, self.weights.T)\n",
    "        \n",
    "        # Actualización de pesos y sesgos (aquí es donde ocurre el \"aprendizaje\")\n",
    "        self.weights -= learning_rate * weights_gradient\n",
    "        self.biases -= learning_rate * biases_gradient\n",
    "        \n",
    "        return input_error\n",
    "\n",
    "# --- Función para cargar y pre-procesar datos ---\n",
    "# Aplanamos la imagen en un vector y la normalizamos.\n",
    "def load_and_preprocess_image(path, size=(64, 64)):\n",
    "    img = Image.open(path).convert('L') # Convertir a escala de grises para simplificar\n",
    "    img = img.resize(size)\n",
    "    img_array = np.array(img, dtype=float)\n",
    "    img_array /= 255.0  # Normalizar píxeles a un rango de 0 a 1\n",
    "    return img_array.flatten().reshape(1, -1) # Aplanar y convertir en vector fila"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Autoencoder\n",
    "class Autoencoder:\n",
    "    def __init__(self, input_size, encoding_size):\n",
    "        self.encoder = DenseLayer(input_size, encoding_size, activation='relu')\n",
    "        self.decoder = DenseLayer(encoding_size, input_size, activation='sigmoid') # Sigmoid para salida entre 0 y 1\n",
    "\n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder.forward(x)\n",
    "        decoded = self.decoder.forward(encoded)\n",
    "        return decoded\n",
    "\n",
    "    def backward(self, output_error, learning_rate):\n",
    "        error = self.decoder.backward(output_error, learning_rate)\n",
    "        self.encoder.backward(error, learning_rate)\n",
    "\n",
    "    def train(self, data, epochs, learning_rate):\n",
    "        # El Autoencoder se entrena con imágenes 'normales'\n",
    "        print(\"Entrenando Autoencoder...\")\n",
    "        for epoch in range(epochs):\n",
    "            total_loss = 0\n",
    "            for x in data: # x es una imagen aplanada\n",
    "                # Forward pass\n",
    "                reconstructed = self.forward(x)\n",
    "                \n",
    "                # Calcular el error (Mean Squared Error)\n",
    "                loss = np.mean((x - reconstructed)**2)\n",
    "                total_loss += loss\n",
    "                \n",
    "                # Backward pass\n",
    "                output_error = 2 * (reconstructed - x) / x.size\n",
    "                self.backward(output_error, learning_rate)\n",
    "            \n",
    "            print(f\"Epoch {epoch+1}/{epochs}, Loss: {total_loss / len(data)}\")\n",
    "\n",
    "# --- Preparación y Entrenamiento del Autoencoder ---\n",
    "\n",
    "NORMAL_IMG_DIR = 'dataset/train/normal/'\n",
    "normal_images = [load_and_preprocess_image(os.path.join(NORMAL_IMG_DIR, f)) for f in os.listdir(NORMAL_IMG_DIR)]\n",
    "\n",
    "IMG_SIZE = 64 * 64\n",
    "ENCODING_SIZE = 32 # Tamaño de la representación comprimida (un hiperparámetro a ajustar)\n",
    "\n",
    "# Crear y entrenar el autoencoder\n",
    "autoencoder = Autoencoder(input_size=IMG_SIZE, encoding_size=ENCODING_SIZE)\n",
    "# autoencoder.train(normal_images, epochs=50, learning_rate=0.01) # Descomentar para entrenar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Clasificador de Anomalías\n",
    "class AnomalyClassifier:\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        self.layer1 = DenseLayer(input_size, hidden_size, activation='relu')\n",
    "        self.layer2 = DenseLayer(hidden_size, num_classes, activation='none') # Salida lineal (logits)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer1.forward(x)\n",
    "        x = self.layer2.forward(x)\n",
    "        return x\n",
    "\n",
    "    def backward(self, error, learning_rate):\n",
    "        error = self.layer2.backward(error, learning_rate)\n",
    "        self.layer1.backward(error, learning_rate)\n",
    "\n",
    "    def train(self, data, labels, epochs, learning_rate):\n",
    "        print(\"Entrenando Clasificador de Anomalías...\")\n",
    "        num_classes = len(np.unique(labels))\n",
    "        for epoch in range(epochs):\n",
    "            total_loss = 0\n",
    "            for x, y_true_idx in zip(data, labels):\n",
    "                # Forward pass\n",
    "                logits = self.forward(x)\n",
    "                \n",
    "                # Softmax y Cross-Entropy Loss\n",
    "                exp_scores = np.exp(logits - np.max(logits))\n",
    "                probs = exp_scores / np.sum(exp_scores)\n",
    "                \n",
    "                loss = -np.log(probs[0, y_true_idx])\n",
    "                total_loss += loss\n",
    "                \n",
    "                # Backward pass (gradiente de cross-entropy con softmax)\n",
    "                probs[0, y_true_idx] -= 1\n",
    "                error = probs / len(data)\n",
    "                \n",
    "                self.backward(error, learning_rate)\n",
    "            \n",
    "            print(f\"Epoch {epoch+1}/{epochs}, Loss: {total_loss / len(data)}\")\n",
    "\n",
    "\n",
    "# --- Preparación y Entrenamiento del Clasificador ---\n",
    "\n",
    "ANOMALY_DIRS = {'incendios': 0, 'basura': 1, 'zonas_secas': 2}\n",
    "anomaly_images = []\n",
    "anomaly_labels = []\n",
    "\n",
    "for label_name, label_idx in ANOMALY_DIRS.items():\n",
    "    dir_path = f'dataset/train/{label_name}/'\n",
    "    for f in os.listdir(dir_path):\n",
    "        anomaly_images.append(load_and_preprocess_image(os.path.join(dir_path, f)))\n",
    "        anomaly_labels.append(label_idx)\n",
    "\n",
    "# Crear y entrenar el clasificador\n",
    "classifier = AnomalyClassifier(input_size=IMG_SIZE, hidden_size=128, num_classes=len(ANOMALY_DIRS))\n",
    "# classifier.train(anomaly_images, anomaly_labels, epochs=100, learning_rate=0.01) # Descomentar para entrenar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Red entrenada\n",
    "def analyze_forest_image(image_path, autoencoder_model, classifier_model, anomaly_threshold=0.015):\n",
    "    \"\"\"\n",
    "    Analiza una imagen usando el sistema de dos etapas.\n",
    "    \"\"\"\n",
    "    # 1. Cargar y pre-procesar la imagen\n",
    "    img_vector = load_and_preprocess_image(image_path)\n",
    "    \n",
    "    # --- Etapa 1: Detector de Anomalías ---\n",
    "    reconstructed_img = autoencoder_model.forward(img_vector)\n",
    "    reconstruction_error = np.mean((img_vector - reconstructed_img)**2)\n",
    "    \n",
    "    print(f\"Error de reconstrucción: {reconstruction_error:.5f}\")\n",
    "    \n",
    "    if reconstruction_error < anomaly_threshold:\n",
    "        return \"Normal\", f\"El bosque parece saludable (Error: {reconstruction_error:.5f})\"\n",
    "    else:\n",
    "        # --- Etapa 2: Clasificador de Problemas ---\n",
    "        logits = classifier_model.forward(img_vector)\n",
    "        \n",
    "        # Obtener la predicción final\n",
    "        predicted_class_idx = np.argmax(logits)\n",
    "        class_names = list(ANOMALY_DIRS.keys())\n",
    "        predicted_class_name = class_names[predicted_class_idx]\n",
    "        \n",
    "        return \"Anomalía Detectada\", f\"Posible problema: {predicted_class_name.capitalize()}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Backend (todavía no lo tenemos xd)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
